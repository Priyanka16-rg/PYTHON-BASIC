{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                                                         KNN & PCA"
      ],
      "metadata": {
        "id": "FX-Tcy64Fy-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Answer- K-Nearest Neighbors (KNN) is a simple, non-parametric, supervised learning algorithm used for both classification and regression that stores training data and predicts a new data point's class or value by finding its \"k\" nearest neighbors in the training set and performing a majority vote (for classification) or an average (for regression) of their labels or values. It is a \"lazy\" or \"instance-based\" learner because it doesn't build a traditional model but rather performs all computation during the prediction phase.\n",
        "\n",
        "**How it works**\n",
        "\n",
        "1. **Store training data:** KNN keeps the entire training dataset in memory.\n",
        "Choose K: Select the number of nearest neighbors (K) to consider for prediction.\n",
        "\n",
        "2. **Calculate distances:** For a new, unlabeled data point, the algorithm calculates the distance between this new point and all points in the training set.\n",
        "\n",
        "3. **Identify neighbors:** The \"k\" data points from the training set that are closest to the new data point are identified as its neighbors.\n",
        "\n",
        "\n",
        "4. **Predict:**\n",
        "\n",
        " * **For Classification:** The class label that is most frequent among the k nearest neighbors is assigned to the new data point.\n",
        "\n",
        " * **For Regression:** The average (or weighted average) of the values of the k nearest neighbors is computed and used as the predicted value for the new data point.\n",
        "\n"
      ],
      "metadata": {
        "id": "P7noWVrQF4P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "The Curse of Dimensionality describes how data becomes sparse and distances between points become less meaningful as the number of features (dimensions) increases, exponentially increasing the volume of the feature space. This adversely affects K-Nearest Neighbors (KNN) by requiring vastly more data for effective pattern identification, leading to increased computational complexity, a higher risk of overfitting, and generally worse performance as distances become less distinct.\n",
        "\n",
        "**What is the Curse of Dimensionality?**\n",
        "\n",
        "**Sparsity:**\n",
        "\n",
        "In a low-dimensional space, data points are relatively close together. As dimensions increase, data points spread out, becoming sparse, with more empty space and fewer points per unit volume.\n",
        "\n",
        "**Volume Growth:**\n",
        "\n",
        "The volume of the feature space grows exponentially with the number of dimensions, making it difficult to adequately sample the entire space.\n",
        "\n",
        "**Distance Concentration:**\n",
        "\n",
        "Distances between points become more uniform in higher dimensions, making it harder to differentiate between nearest and farthest neighbors based on the chosen distance metric.\n",
        "\n",
        "**How does it affect KNN performance?**\n",
        "\n",
        "**Increased Data Requirement:**\n",
        "\n",
        "KNN relies on identifying neighbors. With more dimensions, a much larger dataset is needed to find meaningful neighbors that are close enough to form reliable \"neighborhoods\".\n",
        "\n",
        "**Deteriorating Performance:**\n",
        "\n",
        "As distances become less distinct, the identified nearest neighbors are often very far from the target point, making it harder for KNN to classify correctly.\n",
        "\n",
        "**Computational Complexity:**\n",
        "\n",
        "High-dimensional data increases the computational cost of calculating distances and searching for neighbors, making the algorithm slower and resource-intensive.\n",
        "\n",
        "**Overfitting:**\n",
        "\n",
        "The sparsity and vastness of high-dimensional spaces can lead to overfitting, where the KNN model learns noise instead of actual patterns, resulting in poor generalization to new data.\n",
        "\n",
        "**Strategies to mitigate the curse of dimensionality for KNN: **\n",
        "\n",
        "**Dimensionality Reduction:**\n",
        "\n",
        "Techniques like Principal Component Analysis (PCA) reduce the number of features while preserving essential information, making the data more manageable for KNN.\n",
        "\n",
        "**Feature Selection:**\n",
        "\n",
        "Identifying and using only the most relevant features can reduce dimensionality and improve performance.\n",
        "\n",
        "**Alternative Algorithms:**\n",
        "\n",
        "For high-dimensional data, algorithms like decision trees or support vector machines (SVMs) that are less sensitive to the curse of dimensionality may be more effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "Br360TKCpic2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Answer- Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction by transforming the original, potentially correlated variables into a new set of uncorrelated variables called\n",
        "\n",
        "principal components. These components are linear combinations of the original features and are ordered by the amount of variance they capture in the data—PC1 captures the most, PC2 the next most, and so on\n",
        "\n",
        "**Key aspects of PCA:**\n",
        "\n",
        "\n",
        "* Transformation: The original features are linearly combined into new principal components.\n",
        "\n",
        "\n",
        "* Variance maximization: Each component captures as much variance as possible.\n",
        "\n",
        "\n",
        "* Orthogonality: Components are orthogonal (uncorrelated).\n",
        "\n",
        "\n",
        "* Unsupervised: PCA doesn't consider any target variable or labels for constructing components\n",
        "\n",
        "\n",
        "* Use cases: Commonly applied for visualization, preprocessing, noise reduction, and tackling the curse of dimensionality\n",
        "\n",
        "**What is Feature Selection?**\n",
        "\n",
        "Feature selection involves choosing a subset of the existing original features that are most relevant for a predictive model or analysis. It doesn't create new features—it filters the existing ones.\n",
        "\n",
        "**Key aspects of feature selection:**\n",
        "\n",
        "* Subset selection: Keeps only original features (no transformation).\n",
        "\n",
        "* Supervised or unsupervised: Many methods consider the target variable, especially in classification/regression tasks\n",
        "\n",
        "* Purposes: Simplify models, reduce training time, improve interpretability, and avoid overfitting\n",
        "\n",
        "**PCA vs. Feature Selection: Comparison Table**\n",
        "\n",
        "\n",
        "| Feature                        | PCA                                             | Feature Selection                               |\n",
        "| ------------------------------ | ----------------------------------------------- | ----------------------------------------------- |\n",
        "| Creates new features?          | Yes (linear combinations, principal components) | No (selects from original features only)        |\n",
        "| Uses target variable?          | No (unsupervised)                               | Often yes (supervised), but can be unsupervised |\n",
        "| Interpretability of features?  | Lower — components are harder to interpret      | Higher — original features retain their meaning |\n",
        "| Dimensionality reduction style | Feature extraction                              | Feature subset selection                        |\n",
        "| Common goals                   | Reduce dimensions, capture variance             | Simplify models, improve interpretability       |\n"
      ],
      "metadata": {
        "id": "7vXIXZoHwOWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer- n PCA, eigenvectors represent the directions (principal components) of maximum variance in the data, while eigenvalues indicate the amount of variance captured by each corresponding eigenvector. They are crucial because they rank the principal components by importance, enabling dimensionality reduction by identifying the most significant features that capture the most information, which is vital for simplifying complex datasets.\n",
        "\n",
        "**Eigenvectors:**\n",
        "\n",
        "**What they are:**\n",
        "\n",
        "These are non-zero vectors that point in the directions of maximum variance in the dataset.\n",
        "\n",
        "**What they do:**\n",
        "\n",
        "They define the principal components, which are the new axes that best represent the data.\n",
        "\n",
        "**How to think of them:**\n",
        "\n",
        "Imagine your data plotted in a multidimensional scatterplot; an eigenvector is a particular direction or axis on that plot.\n",
        "\n",
        "**Eigenvalues:**\n",
        "\n",
        "What they are:They are scalar values that measure the magnitude of variance associated with each principal component (eigenvector).\n",
        "\n",
        "What they do: They quantify the amount of information or variance explained by each corresponding eigenvector.\n",
        "\n",
        "How to think of them: A higher eigenvalue corresponds to a more important direction of variance.\n",
        "\n",
        "Why they are important:\n",
        "\n",
        "**Dimensionality Reduction:**\n",
        "\n",
        "By examining the eigenvalues, you can determine which principal components (eigenvectors) explain the most variance and are most significant. You can then discard components with small eigenvalues, thereby reducing the data's dimensionality while retaining most of its important information.\n",
        "\n",
        "**Feature Importance:**\n",
        "\n",
        "Eigenvalues provide a direct measure of how important each feature (principal component) is.\n",
        "\n",
        "**Data Simplification:**\n",
        "\n",
        "PCA uses eigenvectors and eigenvalues to transform the data into a new coordinate system (the principal components) that highlights the most important patterns and reduces noise and redundancy, making the data easier to analyze and use.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kIaGDYsSyhzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "When Principal Component Analysis (PCA) is used as a preprocessing step before a K-Nearest Neighbors (KNN) algorithm in a single pipeline, they complement each other by addressing the \"curse of dimensionality\". PCA reduces the number of features in the data, making the KNN algorithm more efficient and reliable, while still preserving the most important information.\n",
        "\n",
        "**How PCA enhances KNN**\n",
        "\n",
        "A single pipeline that applies PCA before KNN offers several key benefits:\n",
        "\n",
        "**Mitigates the curse of dimensionality**\n",
        "\n",
        "The \"curse of dimensionality\" refers to the problem where the performance of algorithms like KNN deteriorates with a large number of features. In high-dimensional space, data points are sparse, and the distance to the nearest neighbor can become similar to the average distance, making KNN less effective.\n",
        "\n",
        "**How PCA helps:** PCA reduces the number of dimensions by projecting the data onto a smaller number of principal components. This transformation effectively compacts the data into a lower-dimensional space where the concept of distance is more meaningful and reliable for KNN.\n",
        "\n",
        "**Improves computational efficiency**\n",
        "\n",
        "KNN requires calculating the distance from a new data point to every other point in the training set, a process that is computationally expensive and slow with a high number of features.\n",
        "\n",
        "\n",
        "*  **How PCA helps:** By reducing the number of features, PCA dramatically decreases the number of calculations needed for each distance metric, resulting in a significant speedup of the KNN algorithm.\n",
        "\n",
        "**Reduces noise**\n",
        "High-dimensional datasets often contain noisy or redundant features that do not contribute to predictive power but can negatively influence KNN's distance calculations.\n",
        "\n",
        "* **How PCA helps:** The principal components are ordered by the amount of variance they capture. Components with low variance are often associated with noise and are discarded, allowing the pipeline to retain only the most informative features. This makes the KNN classification more robust and less susceptible to misleading, noisy features.\n",
        "\n",
        "**Removes multicollinearity**\n",
        "\n",
        "Multicollinearity occurs when features are highly correlated with one another. In KNN, this means multiple features are essentially capturing the same information, which can skew distance calculations and reduce the model's stability.\n",
        "\n",
        " * **How PCA helps:** PCA creates a new set of orthogonal (uncorrelated) features known as principal components. This naturally eliminates multicollinearity and ensures that each feature in the reduced dataset contributes unique information, leading to a more stable and reliable KNN model.\n",
        "\n",
        "**The pipeline in practice**\n",
        "\n",
        "1. Standardization: The data is first standardized (e.g., using StandardScaler) so that each feature contributes equally to the PCA.\n",
        "\n",
        "2. PCA: The standardized data is passed to the PCA algorithm, which identifies and selects the principal components that capture the majority of the data's variance. The number of components to keep can be chosen based on an explained variance ratio.\n",
        "\n",
        "3. KNN: The transformed, lower-dimensional data is then used to train the KNN algorithm. When a new data point is introduced, it is also transformed using the same PCA model before finding its nearest neighbors.\n",
        "\n",
        "**Potential drawbacks**\n",
        "\n",
        "While highly beneficial, this pipeline has some drawbacks:\n",
        "\n",
        "* **Information loss:** As a lossy compression method, PCA discards some variance from the data. While this often represents noise, it is possible for important nuances to be lost if too many components are discarded, potentially harming the model's accuracy.\n",
        "\n",
        "* **Lack of interpretability:** The principal components are new, abstract features that are linear combinations of the original variables. Interpreting the meaning of a principal component can be challenging, making it difficult to understand exactly which original features are driving the KNN classification."
      ],
      "metadata": {
        "id": "7g4N4XLJ1O8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "Answer- 1. Why Feature Scaling Matters for KNN\n",
        "\n",
        "* The KNN algorithm uses distance metrics (e.g., Euclidean). When features are on different scales, those with larger numeric ranges dominate the distance calculation, skewing predictions. Scaling helps each feature contribute more equally.\n",
        "\n",
        "* Empirical results (on classification tasks involving PCA and logistic regression) show that scaling can dramatically boost accuracy—from just 35.19% to 96.30%, for example.\n",
        "\n",
        "2. What Past Implementations Reveal\n",
        "\n",
        "* In a Medium tutorial using the UCI wine quality dataset and KNN:\n",
        "\n",
        " * Without scaling, the model still reached about 85–86% accuracy, though with imbalance issues—especially poorer classification of minority classes.\n",
        "\n",
        " * When feature selection or PCA weren’t discussed, but scaling combined with other techniques (like balancing or resampling) markedly influences performance.\n",
        "\n",
        "* Other references to KNN on wine data show a wide range of accuracy outcomes, varying from ~72% to near-100%, depending on preprocessing steps—including scaling, hyperparameter tuning, and dealing with class imbalance.\n",
        "\n",
        "3. Proposed Step-By-Step Implementation (Python / scikit-learn)"
      ],
      "metadata": {
        "id": "RizJbLxGDiEw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B0YL-N8TFub0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "MylFZt3DEjAv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "3Y6sbvZkGQ5q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.2%}\")\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEDLlzm1GTYy",
        "outputId": "757d6577-562f-4dc1-8267-4b50f4854fda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 74.07%\n",
            "Accuracy with scaling:    96.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Setup                  | Expected Accuracy  | Notes                                                             |\n",
        "| ---------------------- | ------------------ | ----------------------------------------------------------------- |\n",
        "| Without Scaling        | \\~85–90%           | Some features dominate, distances skewed                          |\n",
        "| With Scaling           | \\~95–97% or higher | Balanced contributions, more reliable distances                   |\n",
        "| Extreme cases (imbal.) | Lower performance  | Minority class misclassified (balanced accuracy low)([Medium][1]) |\n",
        "\n",
        "[1]: https://medium.com/%40caleb.lam10/building-a-knn-model-with-the-uci-wine-dataset-25d89db2003b?utm_source=chatgpt.com \"Building a KNN Model With the UCI Wine Dataset | by Caleb Lam\"\n"
      ],
      "metadata": {
        "id": "iUkq_wTPGfQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "Here’s how you can train a PCA model on the Wine dataset (from sklearn) and print the explained variance ratio of each principal component—a key step to understand how much variance each component captures."
      ],
      "metadata": {
        "id": "HIAz6DCAM91g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-by-Step Guide with Python (using scikit-learn)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "75CgXIQBGWBV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data  # features (13 chemical measurements)\n",
        "y = wine.target  # class labels"
      ],
      "metadata": {
        "id": "QXsMVG8TRfa8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "IPRlgnrfRiTA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Apply PCA using all available components\n",
        "pca = PCA(n_components=X.shape[1])  # 13 components\n",
        "pca.fit(X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "YQ52DD2iRkxC",
        "outputId": "d3fcfaf0-0580-41ba-a855-4b06ba61b3a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=13)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=13)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>PCA</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=13)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Print explained variance ratio\n",
        "for idx, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"Principal Component {idx}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R42F_dS9RoMa",
        "outputId": "4604e1b1-fa96-4e21-842c-0dc08b03cfb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will output how much variance each of the 13 components explains. If you’d like, you can also view the cumulative variance to decide how many components might suffice:"
      ],
      "metadata": {
        "id": "7cOg_Id8Rv5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "cumulative = np.cumsum(pca.explained_variance_ratio_)\n",
        "print(\"Cumulative explained variance:\", cumulative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMLl16vxRrAS",
        "outputId": "057dc505-4858-45d4-f452-1cf62f8fa105"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative explained variance: [0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
            " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Explained Variance Ratio Means**\n",
        "\n",
        "The explained variance ratio indicates the fraction of the dataset’s total variance captured by each principal component. It helps answer questions like: “How many components are needed to preserve, say, 90–95% of the data’s information?”\n",
        "\n",
        "* In general tutorials, it's shown that the first few components can capture most variance.\n",
        "\n",
        "* For instance, one source shared a case where with unscaled data and only two components, the explained variance ratios were approximately [0.99809, 0.00174], meaning the first component captured ~99.8% of the variance\n",
        "\n",
        "\n",
        "* However, when features are properly scaled and all components are used, you'll see a more spread-out distribution depending on the data's structure"
      ],
      "metadata": {
        "id": "qPh1HeiPR5_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "\n",
        "\n",
        "Here’s a Python code snippet to train a K-Nearest Neighbors (KNN) classifier on both:\n",
        "\n",
        "1. The original Wine dataset, and\n",
        "\n",
        "2. The Wine dataset after applying PCA, reduced to the top 2 principal components;\n",
        "\n",
        "then compare their accuracies:"
      ],
      "metadata": {
        "id": "GhSr7ODVSryx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "OBj_t4UCR0HL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "qpsIOwVnUqca"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "klv1FmjvUtnT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "hMnukWwuUv3K"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train KNN on original scaled data\n",
        "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_orig.fit(X_train_scaled, y_train)\n",
        "y_pred_orig = knn_orig.predict(X_test_scaled)\n",
        "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
        "\n",
        "print(f\"KNN accuracy on original data: {acc_orig:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxVm4gkVUyST",
        "outputId": "854c8928-35e7-4290-bf01-9d843e23a54d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN accuracy on original data: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Apply PCA (retain top 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "o6hXXQmQU1Ft"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"KNN accuracy on PCA (top 2 components): {acc_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P6RZXvIU4C5",
        "outputId": "62cd892f-f15a-43c7-a0a7-ffbfd074635d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN accuracy on PCA (top 2 components): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this does:**\n",
        "\n",
        "\n",
        "* Standardizes features to ensure fair distances for KNN.\n",
        "\n",
        "* Trains and evaluates KNN on the full, scaled feature space.\n",
        "\n",
        "* Applies PCA to reduce dimensionality to 2 components and trains KNN on that transformed space.\n",
        "\n",
        "* Prints and compares both accuracy scores.\n",
        "\n",
        "**What the literature says:**\n",
        "\n",
        "* A study in the Journal of Wine Economics found that a KNN classifier trained on just two principal components performed comparably to one trained on all 13 original features, while offering better interpretability\n",
        "\n",
        "* In other domains, PCA has been shown to improve KNN performance by better separating classes, especially when combined with distance-weighted KNN"
      ],
      "metadata": {
        "id": "LAQxWeM9U_K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "To train a K-Nearest Neighbors (KNN) classifier with different distance metrics (Euclidean and Manhattan) on the scaled Wine dataset and compare the results, the following steps are typically performed:\n",
        "\n",
        "**Load and Prepare the Dataset:**\n",
        "\n",
        "\n",
        "*  Load the Wine dataset, which is a common benchmark dataset in machine learning.\n",
        "\n",
        "*  Separate the features (X) from the target variable (y).\n",
        "\n",
        "*  Scale the features using a method like StandardScaler to ensure all features contribute equally to the distance calculations, as KNN is distance-based.\n",
        "\n",
        "**Split Data into Training and Testing Sets:**\n",
        "\n",
        "* Divide the scaled dataset into training and testing sets to evaluate the model's performance on unseen data.\n",
        "\n",
        "**Train KNN with Euclidean Distance:**\n",
        "\n",
        "* Initialize a KNeighborsClassifier with metric='euclidean'.\n",
        "\n",
        "* Train the classifier on the training data.\n",
        "\n",
        "* Make predictions on the test data.\n",
        "\n",
        "* Evaluate the performance using metrics such as accuracy, precision, recall, or F1-score.\n",
        "\n",
        "**Train KNN with Manhattan Distance:**\n",
        "\n",
        "* Initialize another KNeighborsClassifier with metric='manhattan'.\n",
        "\n",
        "* Train this classifier on the same training data.\n",
        "\n",
        "* Make predictions on the test data.\n",
        "\n",
        "* Evaluate its performance using the same metrics.\n",
        "\n",
        "**Compare Results:**\n",
        "\n",
        "* Compare the performance metrics obtained from both models. This comparison reveals which distance metric yielded better results for the given dataset and scaling method. The choice of distance metric can significantly impact KNN's performance, depending on the data's characteristics. Euclidean distance measures the shortest straight-line path, while Manhattan distance measures the sum of absolute differences along each dimension, often referred to as \"city block\" distance."
      ],
      "metadata": {
        "id": "l6qzFEUoVwjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "uV6ZdkT2U6Vu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Prepare the Dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "zjdWZe8AYUy-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Split Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "NyDOFYwqYYaQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train KNN with Euclidean Distance\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "report_euclidean = classification_report(y_test, y_pred_euclidean)\n"
      ],
      "metadata": {
        "id": "BVINwM2UYgiP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train KNN with Manhattan Distance\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "report_manhattan = classification_report(y_test, y_pred_manhattan)\n"
      ],
      "metadata": {
        "id": "QnSVPu0qYoPI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Compare Results\n",
        "print(f\"KNN with Euclidean Distance - Accuracy: {accuracy_euclidean:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsGQZRAOYsXE",
        "outputId": "2043d144-5975-4f2f-e7ef-01b85db38c9e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with Euclidean Distance - Accuracy: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (Euclidean):\\n\", report_euclidean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxfkJvHWY1S8",
        "outputId": "76a647f4-6ce8-45fa-88e3-8ceb08d65673"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Euclidean):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        19\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.96      0.97      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nKNN with Manhattan Distance - Accuracy: {accuracy_manhattan:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwq5YIydZENM",
        "outputId": "3afc5764-9783-4746-e78e-6fb4257a8752"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KNN with Manhattan Distance - Accuracy: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (Manhattan):\\n\", report_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_acyZNQZIlB",
        "outputId": "4389a64e-cc06-495c-f911-310ed757c77a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Manhattan):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        19\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.96      0.97      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "Answer-\n",
        "\n",
        "1.Use PCA for Dimensionality Reduction\n",
        "\n",
        "\n",
        "* Standardize the data: Since PCA is sensitive to scale, each gene's expression values should be centered and scaled (typically z-scores) so that no gene disproportionately influences the results.\n",
        "\n",
        "* Compute principal components: Use either the covariance matrix’s eigen-decomposition or singular value decomposition (SVD) to extract orthogonal components capturing variance in descending order.\n",
        "\n",
        "\n",
        "* Interpretation: The first few components aggregate correlated gene expression patterns—an “eigengene”—reducing noise and redundancy.\n",
        "\n",
        "2.Decide How Many Components to Keep\n",
        "\n",
        "* Explained variance: Plot a scree chart or cumulative variance graph. Choose enough PCs to capture, say, 85–95% of total variance—but stop before including low-variance noise.\n",
        "\n",
        "\n",
        "* Cross-validation: Use CV (e.g., nested CV) to evaluate classification error at varying numbers of PCs. Select the number where validation error is minimized or levels off.\n",
        "\n",
        "3.Apply K-Nearest Neighbors (KNN) for Classification\n",
        "\n",
        "* Train KNN on reduced data: Use the selected PCs as input. KNN classifies each test sample based on the majority class of its ‘k’ nearest neighbors in the reduced space.\n",
        "\n",
        "* Choose k carefully: Tune ‘k’ (e.g., via cross-validation) to balance bias and variance—smaller ‘k’ captures local structure but risks overfitting; larger ‘k’ smooths out noise but may underfit.\n",
        "\n",
        "4.Evaluate the Model\n",
        "\n",
        "* Cross-validation: Use repeated k-fold CV or nested CV to estimate performance robustly.\n",
        "\n",
        "* Performance metrics: Since cancer classification is critical in healthcare, go beyond accuracy—use sensitivity (recall), specificity, precision, F1 score, and ROC-AUC. Consider class imbalance if some cancer types are rarer.\n",
        "\n",
        "* Hold-out validation: If possible, test the final model on an independent external dataset to assess real-world generalizability.\n",
        "\n",
        "5.Justifying the Pipeline to Stakeholders\n",
        "\n",
        "* Addresses overfitting: PCA reduces dimensionality from thousands of genes to a manageable number of components, reducing risk of overfitting with small sample sizes.\n",
        "\n",
        "* Improved robustness and interpretability: PCs summarize correlated gene groups (“eigengenes”), reducing noise and making downstream modeling more stable.\n",
        "\n",
        "\n",
        "* Efficiency and reproducibility: PCA is computationally efficient and deterministic, especially compared to deep learning alternatives.\n",
        "\n",
        "\n",
        "* Transparent and defensible: KNN is a simple, nonparametric classifier. Its decision-making is intuitive (“nearest neighbors”), fostering trust among biomedical stakeholders.\n",
        "\n",
        "* Quantifiable performance: Evaluation through cross-validation and external validation offers transparent, statistically sound measures of accuracy, sensitivity, and generalization.\n",
        "\n",
        "* Flexibility to adapt: You can adjust the number of PCs or k in KNN based on new data or stakeholder requirements, ensuring responsiveness to real-world demands.\n",
        "\n",
        "| Step               | Action                                                                                   |\n",
        "| ------------------ | ---------------------------------------------------------------------------------------- |\n",
        "| **1. Preprocess**  | Standardize gene expression data                                                         |\n",
        "| **2. PCA**         | Compute PCs; retain those capturing optimal variance                                     |\n",
        "| **3. KNN**         | Train KNN using PCs as features                                                          |\n",
        "| **4. Tune & Eval** | Optimize number of PCs, k; use cross-validation and class-aware metrics                  |\n",
        "| **5. Justify**     | Emphasize overfitting reduction, interpretability, efficiency, and validated performance |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sOpoCBeozKfr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_y3-JyzZQ_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}